{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_2_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjL03CKOoEk"
      },
      "source": [
        "Getting started with Spacy<br>\n",
        "Import data.<br>\n",
        "We repeat the preprocessing from the previous homework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOxAAu3cOm9D",
        "outputId": "db00ebcb-a380-4767-fded-4c7ae16b6841"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOc5LrSCPSEC"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_X8KyOPWlJ"
      },
      "source": [
        "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBNxx2f8Pcmq"
      },
      "source": [
        "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKwMlfkaR2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a98667-8584-47bc-da44-3fffcb2237aa"
      },
      "source": [
        "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
        "\n",
        "imported_data = []\n",
        "\n",
        "for datum in myfile:\n",
        "\n",
        "    imported_data.append(datum)\n",
        "\n",
        "myfile.close()\n",
        "\n",
        "# print first line\n",
        "print(imported_data[0])\n",
        "# print how many resumees were read in\n",
        "print(len(imported_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Links\"],\"points\":[{\"start\":1143,\"end\":1239,\"text\":\"https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":438,\"end\":453,\"text\":\"Cisco Networking\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null,\"metadata\":{\"first_done_at\":1527844872000,\"last_updated_at\":1537724086000,\"sec_taken\":0,\"last_updated_by\":\"BIQNZm4INNfvByMqkaVwVt6OZTv2\",\"status\":\"done\",\"evaluation\":\"CORRECT\"}}\n",
            "\n",
            "701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxQojbZyPrkh"
      },
      "source": [
        "import json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axwa389baXDN"
      },
      "source": [
        "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2FaAX-xabje",
        "outputId": "60abcbfb-19b2-4bce-ec2f-c3e92ec05443"
      },
      "source": [
        "## data conversion method\n",
        "def convert_data(data):\n",
        "    \"\"\"\n",
        "    Creates NER training data in Spacy format from JSON dataset\n",
        "    Outputs the Spacy training data which can be used for Spacy training.\n",
        "    \"\"\"\n",
        "    text = data['content']\n",
        "    entities = []\n",
        "    if data['annotation'] is not None:\n",
        "        for annotation in data['annotation']:\n",
        "            # only a single point in text annotation.\n",
        "            point = annotation['points'][0]\n",
        "            labels = annotation['label']\n",
        "            # handle both list of labels or a single label.\n",
        "            if not isinstance(labels, list):\n",
        "                labels = [labels]\n",
        "            for label in labels:\n",
        "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
        "                entities.append((point['start'], point['end'] + 1, label))\n",
        "    return (text, {\"entities\": entities})\n",
        "   \n",
        "## using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
        "## storing the result\n",
        "converted_resumes = []\n",
        "for data in mapped_data:\n",
        "  converted_resumes.append(convert_data(data))\n",
        "## print the number of resumes in converted resumes \n",
        "print(len(converted_resumes))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GapDFuttaqpk"
      },
      "source": [
        "# filter out the resumees whose entities have no entries.\n",
        "converted_complete_resumees = []\n",
        "for converted_resumee in converted_resumes:\n",
        "  entities = converted_resumee[1][\"entities\"]\n",
        "  if entities:\n",
        "    converted_complete_resumees.append(converted_resumee)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-UWPUvfbo0l"
      },
      "source": [
        "Up until now, you could reuse the code from the previous notebook, now, something new comes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYhNshfBbl8d",
        "outputId": "59834f93-eb42-47e0-dc98-5a665432551f"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "print(nlp)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<spacy.lang.en.English object at 0x7f93c3400950>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AqzhVdHb-xO"
      },
      "source": [
        "__nlp__ is Spacy's English language model. For this model, a pretrained NER-model exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDHFtX6Lb-CY",
        "outputId": "46b7aec9-4006-41e2-a9f2-2384efd30950"
      },
      "source": [
        "ner = nlp.get_pipe('ner')\n",
        "labels = ner.labels\n",
        "print(labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3aBMGZbDWwS",
        "outputId": "02c07835-cabd-481d-ac57-57fc8a7b8d76"
      },
      "source": [
        "print(spacy.explain(\"GPE\"))\n",
        "print(spacy.explain(\"FAC\"))\n",
        "print(spacy.explain(\"NORP\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Countries, cities, states\n",
            "Buildings, airports, highways, bridges, etc.\n",
            "Nationalities or religious or political groups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBy9Uo20cZH2"
      },
      "source": [
        "__Question 1__: Explain the labels __GPE__, __FAC__, __NORP__.<br>\n",
        "Which of these labels from __ner__ do you think will Spacy recognize in the resumees?<br>\n",
        "__Answer 1__:<br>\n",
        "\n",
        "*   __GPE__: Countries, cities, states\n",
        "*   __FAC__: Buildings, airports, highways, bridges, etc.\n",
        "*   __NORP__: Nationalities or religious or political groups\n",
        "\n",
        "My prediction is that __ner__ recognizes __GPE__ because of cities of universities and __NORP__ because some people may include their nationality. Not so much __FAC__ as these constructs are not very likely to appear in a resumee.\n",
        "\n",
        "__Task 1__: choose a resumee."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yXayIFffE7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d85233-091b-43bc-b819-93dd3dd08e17"
      },
      "source": [
        "# get a single resume text and print it out.\n",
        "restxt = converted_complete_resumees[0][0]\n",
        "## print it out, removing extraneous spaces\n",
        "print(\"\\n\".join(restxt.split('\\n\\n')))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Afreen Jamadar\n",
            "Active member of IIIT Committee in Third year\n",
            "Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\n",
            "I wish to use my knowledge, skills and conceptual understanding to create excellent team\n",
            "environments and work consistently achieving organization objectives believes in taking initiative\n",
            "and work to excellence in my work.\n",
            "WORK EXPERIENCE\n",
            "Active member of IIIT Committee in Third year\n",
            "Cisco Networking -  Kanpur, Uttar Pradesh\n",
            "organized by Techkriti IIT Kanpur and Azure Skynet.\n",
            "PERSONALLITY TRAITS:\n",
            "• Quick learning ability\n",
            "• hard working\n",
            "EDUCATION\n",
            "PG-DAC\n",
            "CDAC ACTS\n",
            "2017\n",
            "Bachelor of Engg in Information Technology\n",
            "Shivaji University Kolhapur -  Kolhapur, Maharashtra\n",
            "2016\n",
            "SKILLS\n",
            "Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\n",
            "ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\n",
            "ADDITIONAL INFORMATION\n",
            "TECHNICAL SKILLS:\n",
            "• Programming Languages: C, C++, Java, .net, php.\n",
            "• Web Designing: HTML, XML\n",
            "• Operating Systems: Windows […] Windows Server 2003, Linux.\n",
            "• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\n",
            "https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMS6u05UbJwD"
      },
      "source": [
        "Next, we let __nlp__ process that single resumee.<br>\n",
        "__Task 2__: print the results in __doc__. For each result, print the underlying text and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp961WAxfW8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5ec34a-e522-451f-8190-dce146c98e18"
      },
      "source": [
        "doc = nlp(restxt)\n",
        "# Print the results in doc. For each result, print the text and the label.\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Afreen Jamadar PERSON\n",
            "IIIT Committee ORG\n",
            "Third year DATE\n",
            "Sangli ORG\n",
            "Maharashtra - Email ORG\n",
            "IIIT Committee ORG\n",
            "Third year DATE\n",
            "Cisco Networking -   ORG\n",
            "Kanpur GPE\n",
            "Uttar GPE\n",
            "Techkriti IIT Kanpur ORG\n",
            "Azure Skynet WORK_OF_ART\n",
            "2017 DATE\n",
            "Bachelor of Engg in Information Technology ORG\n",
            "Shivaji University ORG\n",
            "Kolhapur PERSON\n",
            "Kolhapur GPE\n",
            "Maharashtra ORG\n",
            "2016 DATE\n",
            "SKILLS ORG\n",
            "Less than 1 year DATE\n",
            "Less than 1 year DATE\n",
            "Linux PERSON\n",
            "Less than 1 year DATE\n",
            "MICROSOFT ORG\n",
            "ACCESS ORG\n",
            "Less than 1 year DATE\n",
            "MICROSOFT ORG\n",
            "Less than 1 year DATE\n",
            "C++ CARDINAL\n",
            "Java PERSON\n",
            "XML ORG\n",
            "Operating Systems ORG\n",
            "2003 DATE\n",
            "Linux PERSON\n",
            "MS Access ORG\n",
            "SQL ORG\n",
            "2008 DATE\n",
            "10 CARDINAL\n",
            "MySql PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xok9PDMplMNe"
      },
      "source": [
        "__Question 2__: How well did Spacy perform at recognizing the labels for this text?<br>\n",
        "__Answer 2__: Not very well, e.g. Java is not a person and C++ not a cardinal.<br>\n",
        "When Spacy predicted the labels for this resumee, a pretrained model was used.<br>\n",
        "__Task 3__: print for this resumee the original labels and their corresponding text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8eIm9lXlznU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1848e7d-eced-4d1c-c294-3392f0b36ac7"
      },
      "source": [
        "# print for that resumee the original labels and their corresponding text.\n",
        "\n",
        "labeled_ents = []\n",
        "\n",
        "for start, end, label in converted_complete_resumees[0][1]['entities']:\n",
        "  labeled_ents.append((label, restxt[start:end]))\n",
        "\n",
        "for ent in labeled_ents:\n",
        "  print(ent)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Email Address', 'indeed.com/r/Afreen-Jamadar/8baf379b705e37c6')\n",
            "('Links', 'https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN')\n",
            "('Skills', 'Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.')\n",
            "('Graduation Year', '2016')\n",
            "('College Name', 'Shivaji University Kolhapur ')\n",
            "('Degree', 'Bachelor of Engg in Information Technology')\n",
            "('Graduation Year', '2017\\n')\n",
            "('College Name', 'CDAC ACTS')\n",
            "('Degree', 'PG-DAC')\n",
            "('Companies worked at', 'Cisco Networking')\n",
            "('Email Address', 'indeed.com/r/Afreen-Jamadar/8baf379b705e37c6')\n",
            "('Location', 'Sangli')\n",
            "('Name', 'Afreen Jamadar')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBHplvL2KHXO"
      },
      "source": [
        "__Question 3__: Compare the performance of the pretrained model __nlp__ and the true labels. Did Spacy perform well? If not, try to explain why.<br>\n",
        "__Answer 3__: Spacy did not perform very well, because the pretrained model interprets the labels differently as they are meant by the resumee annotations and additionally, some labels which were used are not meaningful for resumees at all.<br>\n",
        "__Task 4__: Remember last homework? You chose three labels. Select all the resumees, in which all three labels appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wELEGAoEKDwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe228317-6160-4ce0-d36e-8b10bde0b3b6"
      },
      "source": [
        "# fill in your chosen labels in chosen_entity_labels\n",
        "chosen_entity_labels = [ \"Location\", \"Skills\", \"Degree\" ]\n",
        "\n",
        "## this method gathers all resumes which have all of the chosen entites above.\n",
        "def gather_candidates(dataset,entity_labels):\n",
        "    candidates = list()\n",
        "    for resume in dataset:\n",
        "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
        "        if set(entity_labels).issubset(res_ent_labels):\n",
        "            candidates.append(resume)\n",
        "    return candidates\n",
        "\n",
        "training_data = gather_candidates( converted_complete_resumees, chosen_entity_labels )\n",
        "print(\"Gathered {} training examples\".format(len(training_data)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gathered 447 training examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSDI7dDDLBbW"
      },
      "source": [
        "__Task 5__: Next, we want to remove all other entities, since we only want to train NER for the three entities in __chosen_entity_labels__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBr_VzxLNqe"
      },
      "source": [
        "## filter all annotation based on filter list\n",
        "def filter_ents(ents, filter):\n",
        "    filtered = [ent for ent in ents if ent[2] in filter]\n",
        "    return filtered\n",
        "\n",
        "## Use method above to remove all but relevant (chosen) entity annotations and store in X variable. X shall contain all\n",
        "## the resumees from training_data, but their entity annotations shall be filtered using the function from above.\n",
        "X = [] \n",
        "\n",
        "for text, entities in training_data:\n",
        "  filtered_ents = filter_ents(entities['entities'], chosen_entity_labels)\n",
        "  X.append((text, {'entities': filtered_ents}))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZpyxtX6NteV"
      },
      "source": [
        "__Task 6__: Some resumees cause trouble. We filter these out with the following lines of code.<br>\n",
        "First, use __add_label__ to add your chosen labels to the __ner__ model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqwcgW_xX18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186ef9ee-ea77-4385-aa80-0b926fdc5530"
      },
      "source": [
        "from spacy.gold import GoldParse \n",
        "\n",
        "# add labels \n",
        "ner.add_label( chosen_entity_labels[0] )\n",
        "\n",
        "ner.add_label( chosen_entity_labels[1] )\n",
        "\n",
        "ner.add_label( chosen_entity_labels[2] )\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "good = []\n",
        "\n",
        "for item in X:\n",
        "  \n",
        "  text = nlp.make_doc( item[ 0 ] )\n",
        "\n",
        "  try:\n",
        "    \n",
        "    gold = GoldParse( text, entities = item[ 1 ][ \"entities\" ] )\n",
        "\n",
        "  except:\n",
        "\n",
        "    continue\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    nlp.update( [ text ], [ gold ], drop = 0.3 )\n",
        "\n",
        "  except:\n",
        "\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "\n",
        "    good.append( item )\n",
        "\n",
        "print( \"Number of good samples: \" + str( len( good ) ) )\n",
        "\n",
        "print( \"\" )\n",
        "\n",
        "print( \"\" )\n",
        "\n",
        "print( \"Number of bad samples: \" + str( len( X ) - len( good ) ) )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of good samples: 402\n",
            "\n",
            "\n",
            "Number of bad samples: 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZ93SLzyFIe"
      },
      "source": [
        "For a machine learning model, it is essential to be able to generalize. Only a model, that can generalize well is able to process new data in a meaningful way. Therefore, one usually separates the data set into two sets: the training set and the test set. The training set is used to train the model. The test set is used to evaluate the performance of the model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0XVZPTyyh4T"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split( [ item[ 0 ] for item in good ], [ item[ 1 ] for item in good ], test_size = 0.3 )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AcgIty-1otR"
      },
      "source": [
        "__Task 7__: Complete the following code. Shuffle __new_index__. Create the data sets __x_shuffled__ and __y_shuffled__. Use these to create minibatches, iterate over these minibatches, preprocess the data in a given minibatch using __nlp.make_doc__ and __GoldParse__. Employ __nlp.update__ to update the model using these preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGLUALLr1rEp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "new_index = np.arange( len( x_train ) )\n",
        "\n",
        "x_data = np.array( x_train )\n",
        "\n",
        "y_data = np.array( y_train )\n",
        "\n",
        "minibatch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "for i in range( epochs ):\n",
        "\n",
        "  # shuffle new_index\n",
        "  np.random.shuffle(new_index)\n",
        "\n",
        "  x_shuffled = x_data[new_index] # create x_shuffled from x_data by using the shuffled new_index\n",
        "\n",
        "  y_shuffled = y_data[new_index] # create y_shuffled from y_data by using the shuffled new_index\n",
        "\n",
        "  # divide the data in x_shuffled and y_shuffled into minibatches of identical size\n",
        "  indices = list(range(minibatch_size, len(x_train), minibatch_size))\n",
        "\n",
        "  x_minibatches = np.split(x_shuffled, indices)\n",
        "  y_minibatches = np.split(y_shuffled, indices)\n",
        "\n",
        "  # iterate over these minibatches\n",
        "  for x_minibatch, y_minibatch in zip(x_minibatches, y_minibatches):\n",
        "    # preprocess the data in a minibatch using nlp.make_doc and GoldParse\n",
        "    doc = nlp.make_doc(str(x_minibatch[0]))\n",
        "    \n",
        "    gold = GoldParse(doc, entities=y_minibatch[0]['entities'])\n",
        "\n",
        "    # use these preprocessed data and nlp.update to train the model\n",
        "    nlp.update([doc], [gold])"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSe2-BOYlWN"
      },
      "source": [
        "__Question 4__: Why did we shuffle the data?<br> \n",
        "Why did we employ mini batches?<br>\n",
        "Research the term __epoch__ in machine learning. How many epochs of training do we employ?<br>\n",
        "__Answer 4__: We shuffle the data to reduce the probability of getting mini batches which do not represent the overall data distribution. The use of mini batches relies in the property of being a compromise between SGD and batch gradient decent, means that we have a relatively fast convergence while maintaining enough noise to each gradient update. The __epochs__ are the number of rounds the learning algorithm will work through the entire training dataset to reduce the loss. 20 epochs are employed here.<br>\n",
        "__Task 8__: Next, we choose one resumee and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFGKoHVXY1pL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4042c4f-d270-49c9-8730-2ae69de18b65"
      },
      "source": [
        "resume = x_test[0]\n",
        "\n",
        "print(resume)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rayees Parwez\n",
            "Store Manager - Sales & Operations\n",
            "\n",
            "Mumbai, Maharashtra - Email me on Indeed: indeed.com/r/Rayees-Parwez/a2c576bd71658aca\n",
            "\n",
            "Looking part time job in Mumbai\n",
            "\n",
            "Willing to relocate: Anywhere\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "Store Manager - Sales & Operations\n",
            "\n",
            "Al Dukan Retailing company(FMCG) (Dabbagh Group(Mecca -\n",
            "\n",
            "November 2015 to February 2017\n",
            "\n",
            "Job profile:\n",
            "\n",
            "As a Store Manager-Sales I am responsible for sales, Handling, Motivating, Planning, Staffing,\n",
            "Inventory, Shrinkage & store Hygiene.\n",
            "\n",
            "Job Description:\n",
            ". To achieve Targets\n",
            ". To Maintain Inventory and control Shrinkage\n",
            ". To ensure company policy and procedures are communicated in a better manner\n",
            "Followed & practiced and maintaining the store with good merchandising.\n",
            ". To ensure coordination and out come from team and ensuring team achieves tgts.\n",
            ". To ensure timely review and feedback takes place for all employees.\n",
            "\n",
            "Tasks Undertaken:\n",
            ". Training & Mentoring\n",
            ". Manpower Planning\n",
            ". Merchandising\n",
            ". Operations\n",
            "\n",
            "Area Sales Manager\n",
            "\n",
            "Merik Health Food Pvt Ltd -  Delhi, Delhi -\n",
            "\n",
            "2013 to 2015\n",
            "\n",
            "As an Area Sales Manager the responsibilities include:\n",
            "\n",
            "1. Managing C&FA, Stockists and number of Distributors, Retailers Profitably for achieving\n",
            "the overall business targets.\n",
            "\n",
            "2. Setting and executing plans for Monthly sales targets according to the location and potential\n",
            "of the stores.\n",
            "\n",
            "3. Keeping in consideration the offerings and strategies of competition to stay ahead.\n",
            "\n",
            "https://www.indeed.com/r/Rayees-Parwez/a2c576bd71658aca?isid=rex-download&ikw=download-top&co=IN\n",
            "\n",
            "\n",
            "4. Hiring and Training Staff according to the policies of company.\n",
            "\n",
            "5. Motivating, encouraging, enabling staff for career growth and professional grooming.\n",
            "\n",
            "6. Offering an environment of centralized organizational structure for effective communication\n",
            "among staff.\n",
            "\n",
            "7. Implementing the Rules and Regulations set by management at every store for unanimity\n",
            "across the board.\n",
            "\n",
            "8. Regularly visiting store for ensuring the proper guidelines are being followed by store staff.\n",
            "\n",
            "9. Training staff for such customer service which helps to achieve the utmost customer\n",
            "satisfaction.\n",
            "\n",
            "Assistant Manager(Department Manager)\n",
            "\n",
            "Reliance Digital Retail Ltd -  Kolkata, West Bengal -\n",
            "\n",
            "2010 to 2013\n",
            "\n",
            "for CDIT\n",
            "Job Profile:\n",
            "As a Department Manager i am responsible for Sales, Profitability, Handling, Motivating and\n",
            "Staffing the team, planning and acheiveing budgeted target and as MOD(Manager on Duty)\n",
            "responsible for store opening and closing and handling customer issues and resolving them on\n",
            "time\n",
            "\n",
            "Job Description:\n",
            "• To achieve the targets set for the store.\n",
            "• To control shrinkage in the store.\n",
            "• To ensure display of merchandise as per planogram.\n",
            "• To ensure the company policy and procedures are communicated in a timely manner to the\n",
            "subordinates and followed accordingly.\n",
            "• To communicate actual store sales compared to set targets to all relevant store employees on\n",
            "a daily basis and ensure achievement of targets.\n",
            "• To keep all the subordinates aware of all the in-store and competitors offers and promotions.\n",
            "• To ensure staff presentation is in line with company preferred standards.\n",
            "• To manage and motivate the store team to increase sales and ensure efficiency in customer\n",
            "service.\n",
            "• To ensure that the sales staff have all the required product knowledge.\n",
            "• To ensure customer service & product training is provided to all store employees.\n",
            "• To ensure timely review and feedback takes place for all store employees.\n",
            "\n",
            "Tasks Undertaken:\n",
            "• Catchment Analysis\n",
            "• Market Mapping\n",
            "• Price and Product Tracking\n",
            "• Competition Mapping(Ad's & Promotions Tracker)\n",
            "\n",
            "\n",
            "\n",
            "• Manpower & Shift Planning\n",
            "• Store Planning & Merchandising\n",
            "\n",
            "Business Development Officer\n",
            "\n",
            "COUNTRY CLUB INDIA LIMITED -  Kolkata, West Bengal -\n",
            "\n",
            "March 2007 to November 2009\n",
            "\n",
            "Key Responsibilities:\n",
            "Report to Manager & General Manager.\n",
            "Monitor Sr. Sales Exe, Sales Exe, Tele caller & Team Leader.\n",
            "Coordinate with Administration Department and operations departments.\n",
            "Prepare and update the contract rates and service contracts with client.\n",
            "Maintain inventory system and keep proper records of incoming and outgoing prospective\n",
            "customers.\n",
            "Make timely collection of payments from the end of the month as a target basis.\n",
            "Maintain records of incoming and outgoing mails.\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "Diploma in Deck Cadet in Deck Cadet\n",
            "\n",
            "Academy of Maritime Studies Pvt ltd New Delhi -  New Delhi, Delhi\n",
            "\n",
            "2010\n",
            "\n",
            "B.A\n",
            "\n",
            "(B.S.K) College ( Vinoba Bhave University)\n",
            "\n",
            "2004\n",
            "\n",
            "High Secondary\n",
            "\n",
            "2001\n",
            "\n",
            "Diploma in computer application\n",
            "\n",
            "Haji Qadam Rasool School\n",
            "\n",
            "1998\n",
            "\n",
            "SKILLS\n",
            "\n",
            "Business Development (2 years), Relationship management (Less than 1 year)\n",
            "\n",
            "CERTIFICATIONS/LICENSES\n",
            "\n",
            "Human resource management\n",
            "\n",
            "July 2018 to September 2018\n",
            "\n",
            "\n",
            "\n",
            "Completed (SHRM) Strategic of Human resource management\n",
            "\n",
            "ADDITIONAL INFORMATION\n",
            "\n",
            "Areas of Expertise\n",
            "\n",
            "Retail & operations \n",
            "Area Management \n",
            "⇒ Business Development\n",
            "⇒ Relationship management\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYNKx837bJwI"
      },
      "source": [
        "__Task 9__: we process this resumee using __nlp__. Print for all items in __doc.ents__ the predicted label and the corresponding text. Then print the correct labels and their corresponding text for that resumee with data from __y_test__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arAyrdyfbnQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c6f5b8-971e-438a-be35-b1dfe77a007e"
      },
      "source": [
        "doc = nlp( resume )\n",
        "# print for all the items in doc.ents the predicted label and the corresponding text\n",
        "for item in doc.ents:\n",
        "  print(item.label_, \":\", item.text)\n",
        "\n",
        "# print the correct labels and their corresponding text for that resumee with data from y_test\n",
        "print()\n",
        "labeled_ents = []\n",
        "\n",
        "for start, end, label in y_test[0]['entities']:\n",
        "  labeled_ents.append((label, resume[start:end]))\n",
        "\n",
        "for ent in labeled_ents:\n",
        "  print(ent)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Location : Mumbai\n",
            "Location : Mumbai\n",
            "Location : Delhi\n",
            "Location : Delhi\n",
            "Location : Kolkata\n",
            "Location : Kolkata\n",
            "Degree : Diploma in Deck Cadet in Deck Cadet\n",
            "Location : Delhi\n",
            "Location : New Delhi\n",
            "Degree : B.A\n",
            "Degree : Diploma in computer application\n",
            "Skills : Business Development\n",
            "Skills : Relationship management\n",
            "Skills : Business Development\n",
            "\n",
            "('Skills', 'Relationship management')\n",
            "('Skills', 'Business Development')\n",
            "('Skills', 'Relationship management')\n",
            "('Skills', 'Business Development')\n",
            "('Degree', 'B.A')\n",
            "('Degree', 'Diploma in Deck Cadet in Deck Cadet')\n",
            "('Skills', 'Business Development')\n",
            "('Location', 'Mumbai')\n",
            "('Location', 'Mumbai')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjZw4RVeB87"
      },
      "source": [
        "__Question 5__: What labels did the model predict correctly?<br> \n",
        "Where appeared problems?<br> \n",
        "How can you explain the problems?<br>\n",
        "__Answer 5__: The model predicted \"Degree\" correctly, misslabeled some \"Locations\" and did not recognize every \"Skill\". <br>\n",
        "__Question 6__: We can evaluate the performance of the model using 4 metrics: the __Accuracy__, the __Precision__, the __Recall__ and __F1__.<br>\n",
        "Inform yourself on these metrics. How are they defined? Explain the concept of __True Positive__, __True Negative__, __False Positive__ and __False Negative__. Use these to define  the __Accuracy__, the __Precision__, the __Recall__ and __F1__, and also give the formula for each of these.<br>\n",
        "__Answer 7__: <br>\n",
        "*   __True Positive__: An outcome where the model *correctly* predicts the *positive* class.\n",
        "*   __True Negative__: An outcome where the model *correctly* predicts the *negative* class.\n",
        "*   __False Positive__: An outcome where the model *incorrectly* predicts the *positive* class.\n",
        "*   __False Negative__: An outcome where the model *incorrectly* predicts the *negative* class.\n",
        "*   __Accuracy__: Measures the proportion of a model's correct predictions. <br>\n",
        " *(TP + TN) / (TP + TN + FP + FN)*\n",
        "*   __Precision__: Measures the proportion positive identifications that were actually correct. <br>\n",
        " *TP / (TP + FP)*\n",
        "*   __Recall__: Measures the proportion of actual positives that were identified correctly.<br>\n",
        " *TP / (TP + FN)*\n",
        "*   __F1__: An overall measure of a model’s accuracy that combines precision and recall. <br>\n",
        " *2 * (Precision * Recall) / (Precision + Recall)*\n",
        "\n",
        "\n",
        "__Task 10__: Complete the following code. Call __make_bilou_df__ with a resume from the test set and store result in __bilou_df__ variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5M-ArsDtzFd",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f6911a62-735f-4cfe-de98-693eb95420f6"
      },
      "source": [
        "from spacy.gold import biluo_tags_from_offsets\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
        "def make_bilou_df(nlp,resume):\n",
        "    \"\"\"\n",
        "    param nlp - a trained spacy model\n",
        "    param resume - a resume from our train or test set\n",
        "    \"\"\"\n",
        "    doc = nlp(resume[0])\n",
        "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
        "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
        "\n",
        "    \n",
        "    doc_tokens = [tok.text for tok in doc]\n",
        "    bilou_df = pd.DataFrame()\n",
        "    bilou_df[\"Tokens\"] =doc_tokens\n",
        "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
        "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
        "    bilou_df[\"True\"] = bilou_ents_true\n",
        "    return bilou_df\n",
        "\n",
        "## call method above with a resume from test set and store result in bilou_df variable.\n",
        "bilou_df = make_bilou_df( nlp, (x_test[0], y_test[0]) )\n",
        "display(bilou_df)  "
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rayees</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Parwez</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Store</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Manager</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>Development</td>\n",
              "      <td>L-Skills</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>916</th>\n",
              "      <td>⇒</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>917</th>\n",
              "      <td>Relationship</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>management</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>919 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Tokens Predicted      True\n",
              "0          Rayees         O         O\n",
              "1          Parwez         O         O\n",
              "2                         O         O\n",
              "3           Store         O         O\n",
              "4         Manager         O         O\n",
              "..            ...       ...       ...\n",
              "914   Development  L-Skills  L-Skills\n",
              "915                       O         O\n",
              "916             ⇒         O         O\n",
              "917  Relationship         O  B-Skills\n",
              "918    management         O  L-Skills\n",
              "\n",
              "[919 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOGZt0FibJwJ"
      },
      "source": [
        "Inform yourself on the [BILUO](https://spacy.io/usage/linguistic-features#accessing-ner) scheme.<br>\n",
        "__Question 7__: Why do you think is it better to tag entities using this scheme (consider names of humans, descriptions of items in a shop)?<br>\n",
        "__Answer 7__: Get a more intuitive feeling of wrong predictions.<br>\n",
        "__Task 11__: employ pandas dataframe api to get a subset where predicted and true labels are the same. Compute the accuracy using the formula you researched above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWRI3IfluPD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d3f22c-4acf-4bd6-af43-44e6d41a45e6"
      },
      "source": [
        "## bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
        "same_df = bilou_df[bilou_df['Predicted'] == bilou_df['True']]\n",
        "## compute the accuracy\n",
        "accuracy = len(same_df) / len(bilou_df)\n",
        "\n",
        "print(\"Accuracy on one resume: \",accuracy)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on one resume:  0.9836779107725789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P4EgefioE-Q"
      },
      "source": [
        "The __accuracy__ is not 100%. Therefore, we want to have a look at those tokens, where the predicted and the true value differ.<br>\n",
        "__Task 12__: create a dataframe diff_df where the predicted values and the true values differ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHFX1cMn-r6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "935ddaba-ead1-41cd-fdaa-8694df634057"
      },
      "source": [
        "# create a dataframe diff_df where the predicted values and the true values differ\n",
        "diff_df = bilou_df[bilou_df['Predicted'] != bilou_df['True']]\n",
        "display(diff_df)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>U-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>U-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>U-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>Business</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>685</th>\n",
              "      <td>Development</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>U-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>U-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>New</td>\n",
              "      <td>B-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>L-Location</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>Diploma</td>\n",
              "      <td>B-Degree</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>in</td>\n",
              "      <td>I-Degree</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>computer</td>\n",
              "      <td>I-Degree</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>application</td>\n",
              "      <td>L-Degree</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>917</th>\n",
              "      <td>Relationship</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>management</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Tokens   Predicted      True\n",
              "196         Delhi  U-Location         O\n",
              "198         Delhi  U-Location         O\n",
              "393       Kolkata  U-Location         O\n",
              "684      Business           O  B-Skills\n",
              "685   Development           O  L-Skills\n",
              "694       Kolkata  U-Location         O\n",
              "815         Delhi  U-Location         O\n",
              "818           New  B-Location         O\n",
              "819         Delhi  L-Location         O\n",
              "844       Diploma    B-Degree         O\n",
              "845            in    I-Degree         O\n",
              "846      computer    I-Degree         O\n",
              "847   application    L-Degree         O\n",
              "917  Relationship           O  B-Skills\n",
              "918    management           O  L-Skills"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWvyiCokonUI"
      },
      "source": [
        "Since we only considered one resumee, we now make this comparison for the whole test set.<br>\n",
        "__Task 13__: Complete the following code for the computation of the overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidqT9GjovAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea13c70d-738a-4045-e165-d7799a8b5264"
      },
      "source": [
        "doc_accuracy = []\n",
        "\n",
        "for i in range( len( x_test ) ):\n",
        "\n",
        "  resume = (x_test[i], y_test[i])\n",
        "\n",
        "  bilou_df = make_bilou_df(nlp, resume)\n",
        "\n",
        "  same_df = bilou_df[bilou_df['Predicted'] == bilou_df['True']]\n",
        "\n",
        "  doc_accuracy.append( len(same_df) / len(bilou_df) )\n",
        "\n",
        "total_acc = np.mean( doc_accuracy )\n",
        "print(\"Accuracy: \",total_acc)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9040195953284105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL7GE3RQpzbg"
      },
      "source": [
        "So we got an __accuracy__ of about 90% on average. This is quite good considering, that we only considered about 300 cases for training.<br>\n",
        "__Task 14__: Next, we want to find out, what the model did, when it went wrong. We only consider 5 resumees.<br>\n",
        "Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y02Pkxriq8GM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f0f1c9-565b-455a-d155-6c5e67324d77"
      },
      "source": [
        "for i in range( 5 ):\n",
        "\n",
        "  resume = (x_test[i], y_test[i])\n",
        "\n",
        "  bilou_df = make_bilou_df(nlp, resume)\n",
        "\n",
        "  difference_df = bilou_df[bilou_df['Predicted'] != bilou_df['True']]\n",
        "\n",
        "  # print, where the labels from Spacy and the annotation differ. Print the text, the predicted and the true labels.\n",
        "  print(difference_df, '\\n')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Tokens   Predicted      True\n",
            "196         Delhi  U-Location         O\n",
            "198         Delhi  U-Location         O\n",
            "393       Kolkata  U-Location         O\n",
            "684      Business           O  B-Skills\n",
            "685   Development           O  L-Skills\n",
            "694       Kolkata  U-Location         O\n",
            "815         Delhi  U-Location         O\n",
            "818           New  B-Location         O\n",
            "819         Delhi  L-Location         O\n",
            "844       Diploma    B-Degree         O\n",
            "845            in    I-Degree         O\n",
            "846      computer    I-Degree         O\n",
            "847   application    L-Degree         O\n",
            "917  Relationship           O  B-Skills\n",
            "918    management           O  L-Skills \n",
            "\n",
            "        Tokens Predicted      True\n",
            "162       Hard         O  B-Skills\n",
            "163    working         O  L-Skills\n",
            "167  Excellent         O  B-Skills\n",
            "168   learning         O  I-Skills\n",
            "169    ability         O  I-Skills\n",
            "..         ...       ...       ...\n",
            "313   Computer  I-Skills         O\n",
            "314     Skills  I-Skills         O\n",
            "315             I-Skills         O\n",
            "316          •  I-Skills         O\n",
            "317      Basic  I-Skills  B-Skills\n",
            "\n",
            "[79 rows x 3 columns] \n",
            "\n",
            "          Tokens   Predicted True\n",
            "281    Bengaluru  U-Location    O\n",
            "291         CBSE           O    -\n",
            "292           in           O    -\n",
            "293  Mathematics           O    -\n",
            "294          and           O    -\n",
            "..           ...         ...  ...\n",
            "426            ,    I-Skills    -\n",
            "427                 I-Skills    -\n",
            "428     Tortoise    I-Skills    -\n",
            "429          SVN    I-Skills    -\n",
            "430            .    L-Skills    -\n",
            "\n",
            "[120 rows x 3 columns] \n",
            "\n",
            "       Tokens   Predicted      True\n",
            "4    Kharghar           O         -\n",
            "5        Navi  B-Location         -\n",
            "6      mumbai  L-Location         -\n",
            "31       Navi  B-Location         O\n",
            "32     Mumbai  L-Location         O\n",
            "90     Mumbai  U-Location         O\n",
            "94     Mumbai  U-Location         O\n",
            "103  Computer           O  B-Skills\n",
            "104  software           O  I-Skills\n",
            "105   diploma           O  I-Skills\n",
            "106      done           O  L-Skills \n",
            "\n",
            "            Tokens Predicted        True\n",
            "9          Trichur         O  B-Location\n",
            "10               ,         O  L-Location\n",
            "63         Trichur         O  B-Location\n",
            "64               ,         O  L-Location\n",
            "80         Trichur         O  B-Location\n",
            "81               ,         O  L-Location\n",
            "107          B.Com  B-Degree           -\n",
            "108             in  I-Degree           -\n",
            "109       Computer  I-Degree           -\n",
            "110    Application  L-Degree           -\n",
            "111                        O           -\n",
            "124           SSLC  U-Degree           O\n",
            "135        SOLVING  L-Skills    I-Skills\n",
            "136              (         O    I-Skills\n",
            "137           Less         O    I-Skills\n",
            "138           than         O    I-Skills\n",
            "139              1         O    I-Skills\n",
            "140           year         O    I-Skills\n",
            "141              )         O    L-Skills\n",
            "150      ABILITIES         O           -\n",
            "151              :         O           -\n",
            "152                        O           -\n",
            "153              •         O           -\n",
            "154           Good         O           -\n",
            "155  Communication         O           -\n",
            "156          Skill         O           -\n",
            "157                        O           -\n",
            "158              •         O           -\n",
            "159        Sincere         O           -\n",
            "160                        O           -\n",
            "161              •         O           -\n",
            "162           Hard         O           -\n",
            "163        working         O           -\n",
            "164                        O           -\n",
            "165              •         O           -\n",
            "166     Leadership         O           -\n",
            "167          skill         O           -\n",
            "168                        O           -\n",
            "169              •         O           -\n",
            "170       Pleasing         O           -\n",
            "171    personality         O           -\n",
            "172                        O           -\n",
            "173              •         O           -\n",
            "174        Problem         O           -\n",
            "175        solving         O           -\n",
            "176     capability         O           - \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r21TdjPrt7P"
      },
      "source": [
        "__Question 8__: What was predicted, when the prediction differed from the true label?<br>\n",
        "What do you think is necessary for computing the accuracy on token level?<br> \n",
        "What is the advantage of computing the accuracy on token level?<br>\n",
        "__Answer 8__: The tokens were predicted as false negatives and false positive so we need to compute the precision and recall to get a better picture of the distribution.<br>\n",
        "__Task 15__: Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZGOVg2U20V1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "17461534-200b-4be0-e6a0-6e716a328dec"
      },
      "source": [
        "## cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
        "data = []\n",
        "for label in chosen_entity_labels:\n",
        "\n",
        "  ## variables to store results for all resumes for one entity type\n",
        "  true_positives = 0\n",
        "  false_positives = 0\n",
        "  false_negatives = 0\n",
        "  for i in range( len( x_test ) ):\n",
        "    ## use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives,\n",
        "    ## and false negatives. \n",
        "\n",
        "    resume = (x_test[i], y_test[i])\n",
        "    \n",
        "    tres_df = make_bilou_df(nlp, resume)\n",
        "\n",
        "    ## calculate true false positives and false negatives for each resume\n",
        "\n",
        "    tres_label_df = tres_df[tres_df['Predicted'].str.contains(label) | tres_df['True'].str.contains(label)]\n",
        "\n",
        "    tp = len(tres_label_df[tres_label_df['Predicted'] == tres_label_df['True']])\n",
        "    \n",
        "    fp = len(tres_label_df[(tres_label_df['Predicted'].str.contains(label)) \n",
        "      & (tres_label_df['True'].str.contains(label) == False)])\n",
        "    \n",
        "    fn = len(tres_label_df[(tres_label_df['Predicted'].str.contains(label) == False) \n",
        "      & (tres_label_df['True'].str.contains(label))])\n",
        "\n",
        "    ## aggregate result for each resume to totals\n",
        "    true_positives = true_positives + tp\n",
        "    false_positives = false_positives + fp\n",
        "    false_negatives = false_negatives + fn\n",
        "    \n",
        "  print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
        "  \n",
        "  ## Use the formulas you learned to calculate metrics and print them out\n",
        "  ## also: prevent division by zero without raising errors. Explain your choice\n",
        "\n",
        "  if (true_positives == 0):\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    f1 = 0\n",
        "  else:\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  row = [label,precision,recall,f1]\n",
        "  data.append(row)\n",
        "\n",
        "## make pandas dataframe with metrics data. Use the chosen entity labels as an index, and the metric names as columns. \n",
        "metric_df = pd.DataFrame( data, columns = [ \"Label\", \"Precision\", \"Recall\", \"F1\" ] )\n",
        "display(metric_df)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 'Location' tp: 265 fp: 184 fn: 63\n",
            "For label 'Skills' tp: 1186 fp: 1916 fn: 1523\n",
            "For label 'Degree' tp: 432 fp: 352 fn: 59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Location</td>\n",
              "      <td>0.590200</td>\n",
              "      <td>0.807927</td>\n",
              "      <td>0.682111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Skills</td>\n",
              "      <td>0.382334</td>\n",
              "      <td>0.437800</td>\n",
              "      <td>0.408191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Degree</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.879837</td>\n",
              "      <td>0.677647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label  Precision    Recall        F1\n",
              "0  Location   0.590200  0.807927  0.682111\n",
              "1    Skills   0.382334  0.437800  0.408191\n",
              "2    Degree   0.551020  0.879837  0.677647"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMKMlJswAqUZ"
      },
      "source": [
        "__Question 9__: Explain from these statistics how well __nlp__ performs.<br>\n",
        "__Answer 9__: The overall performance is not very good, e.g. for \"Skills\" only around 40% (F1) were predicted correctly. <br>\n",
        "__Task 16__: Compute for each metric (Precision, Recall, F1) the mean values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_gEZTQy5KTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c89e80-e4c1-47c5-9cbe-4973adc7b5cd"
      },
      "source": [
        "for label in [ \"Precision\", \"Recall\", \"F1\" ]:\n",
        "    \n",
        "    # Compute mean and print\n",
        "    print(label, \": \", np.mean(metric_df[label]))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision :  0.5078516105587408\n",
            "Recall :  0.7085212742166961\n",
            "F1 :  0.5893163673819023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q1rvvhfBoN9"
      },
      "source": [
        "__Question 10__: What do you learn, when you compare the performance of the model on the token level with the performance of the model on the global level from above?<br>\n",
        "__Answer 10__: The performance for each token varies regarding the precision as well as recall. <br>\n",
        "Next, we prepare data for flair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkzBYIlDbLe"
      },
      "source": [
        "train = [ [ x_train[ i ], y_train[ i ] ] for i in range( len( x_train ) ) ]\n",
        "\n",
        "test = [ [ x_test[ i ], y_test[ i ] ] for i in range( len( x_test ) ) ]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcvkzYubJwN"
      },
      "source": [
        "__Task 17__: Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBfdFr1qNKBv"
      },
      "source": [
        "# prepare data\n",
        "training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
        "\n",
        "test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
        "\n",
        "\n",
        "# set up paths\n",
        "path_to_training_file = os.getcwd() + \"/training_data.csv\"\n",
        "\n",
        "path_to_test_file = os.getcwd() + \"/test_data.csv\"\n",
        "\n",
        "\n",
        "\n",
        "# make sure, that if the corresponding files exist, they are emptied\n",
        "if os.path.isfile( path_to_training_file ):\n",
        "\n",
        "  open( path_to_training_file, \"w\" ).close()\n",
        "\n",
        "if os.path.isfile( path_to_test_file ):\n",
        "\n",
        "  open( path_to_test_file, \"w\" ).close()\n",
        "\n",
        "\n",
        "# open empty files\n",
        "training_file = open( path_to_training_file, \"a\", encoding = \"utf-8\" )\n",
        "    \n",
        "test_file = open( path_to_test_file, \"a\", encoding = \"utf-8\" )\n",
        "\n",
        "\n",
        "for item in training_data_as_bilou:\n",
        "  # remove all tokens like \"\", \" \", \"\\n\" by ignoring them\n",
        "  # for all other tokens do the following:\n",
        "  # create a string s: s = token + \" \" + label + \"\\n\"\n",
        "  # if the label is \"-\", then write s = token + \" O\\n\"\n",
        "  #\n",
        "  # write this newly created string to file\n",
        "  # if this newly created string contains \".\", then also write a \n",
        "  # newline to file that only contains \"\\n\"\n",
        "  #\n",
        "  # Using this scheme, each line in the resulting files corresponds either to an empty line or a token.\n",
        "  # Flair assembles a block of nonempty lines into a sentence. Therefore, the empty line\n",
        "  # is a signal for Flair that the current sentence is finished. Therefore, we extracted\n",
        "  # the whitespaces above.\n",
        "  for token, label in zip(item['Tokens'], item['True']):\n",
        "\n",
        "    if token in [\"\", \" \", \"\\n\"]:\n",
        "      continue\n",
        "\n",
        "    s = \"\"\n",
        "    if label == \"-\":\n",
        "      s = token + \" O\\n\"\n",
        "    else:\n",
        "      s = token + \" \" + label + \"\\n\"\n",
        "\n",
        "    training_file.write(s)\n",
        "    \n",
        "    if \".\" in s:\n",
        "      training_file.write(\"\\n\")\n",
        "\n",
        "for item in test_data_as_bilou:\n",
        "\n",
        "  for token, label in zip(item['Tokens'], item['True']):\n",
        "\n",
        "    if token in [\"\", \" \", \"\\n\"]:\n",
        "      continue\n",
        "\n",
        "    s = \"\"\n",
        "    if label == \"-\":\n",
        "      s = token + \" O\\n\"\n",
        "    else:\n",
        "      s = token + \" \" + label + \"\\n\"\n",
        "\n",
        "    test_file.write(s)\n",
        "    \n",
        "    if \".\" in s:\n",
        "      test_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "training_file.close()\n",
        "\n",
        "test_file.close()"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V5trnr4gXa9"
      },
      "source": [
        "Start Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630pwinWhXWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b6e8fc-3aa6-4283-9c55-c33d16bc5376"
      },
      "source": [
        "pip install flair"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 5.0MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Collecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 19.4MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 34.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 33.4MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 30.2MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=1e3d8cbb970fc7d6cbeb4ff161a2aa0a48814dd23c0dac7caf196642ad9a5587\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: sqlitedict, segtok, mpld3, ftfy, langdetect, overrides\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=b06c193f94720ab52b437d145dd04f948e868332abd2839ddd7bb4a6f4a480a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=14be84b2aeee251dd93e936dc666389e5fe1734ef446f84d2639625fccd36135\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=579f1d0a0e6ca591aeec6e8068bd9d87ce74a624a2480a62f2aa192c7f469bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=bd093b57f7c4ae305e8e7869ae13e53900eb5e2bbe9b931d18524b34efea5e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=549f3683f7c7e33547895e0b5d5451eb43d75d42b8a4b15ececb73edbb7b0011\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=5b56a42ac5ae49d62708d0fa0389a74b863dcd7c02a7d7e171a6e12c2dd4bb4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built sqlitedict segtok mpld3 ftfy langdetect overrides\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement importlib-metadata<4.0.0,>=3.7.0, but you'll have importlib-metadata 4.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sqlitedict, huggingface-hub, segtok, sacremoses, tokenizers, transformers, mpld3, sentencepiece, gdown, deprecated, bpemb, ftfy, janome, torch, overrides, konoha, langdetect, flair\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.1 gdown-3.12.2 huggingface-hub-0.0.8 janome-0.4.1 konoha-4.6.4 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.2 torch-1.7.1 transformers-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKoPUsQGgaEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4b6426-d91d-4bac-8a2f-a88c5949ed93"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "\n",
        "# your training file name\n",
        "data_folder = os.getcwd() \n",
        "\n",
        "train_file = \"training_data.csv\"\n",
        "\n",
        "# your test file name\n",
        "test_file = \"test_data.csv\"\n",
        "\n",
        "# when we wrote the data files, each row was either empty to signal the end\n",
        "# of a sentence to Flair, or the line contained a token, a white space and a label.\n",
        "# In the next line, we assign, that the token is the \"text\", and that the label is \n",
        "# \"ner\" label\n",
        "columns =  {0: 'text', 1: 'ner'}\n",
        "\n",
        "## Now load our csv into flair corpus\n",
        "corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n",
        "                                               train_file=train_file,\n",
        "                                               test_file=test_file)\n",
        "print(corpus)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-20 07:50:30,229 Reading data from /content/gdrive/My Drive/flair\n",
            "2021-05-20 07:50:30,231 Train: /content/gdrive/My Drive/flair/training_data.csv\n",
            "2021-05-20 07:50:30,239 Dev: None\n",
            "2021-05-20 07:50:30,242 Test: /content/gdrive/My Drive/flair/test_data.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 7975 train + 886 dev + 3032 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}