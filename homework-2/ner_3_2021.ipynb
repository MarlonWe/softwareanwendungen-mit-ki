{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_3_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CyQGFg4haX"
      },
      "source": [
        "Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRxrYeEV4gVL",
        "outputId": "fcb0e2c9-05e9-48f9-b6bc-b693ecd72e30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Toer0O450W"
      },
      "source": [
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH08d9VJ5Ife"
      },
      "source": [
        "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug6zYGQ85KbW",
        "outputId": "0b4d0bc7-2446-4346-c4aa-aa09a15d41ab"
      },
      "source": [
        "pip install flair"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.9)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.1)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair) (3.12.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair) (6.0.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair) (1.2.12)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (2.25.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.2)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdwhJEXL54Aw"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import WIKINER_ENGLISH"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGeR8FJ8Bsj"
      },
      "source": [
        "Next, we create __wikiner_corpus__, an instance of the class __Corpus__.<br>\n",
        "Read [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md) the documentation of __Corpus__.<br>\n",
        "__Question 1__: explain, what the __WIKINER__ corpus is.<br>\n",
        "__Answer 1__: A corpus is a list of train, validation and testing sentences. __WIKINER__ corresponds to a NER dataset which was automatically generated from Wikipedia.<br>\n",
        "Then, we create __tag_dictionary__ which is an __BILUO__-__NER__-encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hqOdEn7565N",
        "outputId": "c3a5c9e1-3aa4-4fd9-f05f-e515a1da621e"
      },
      "source": [
        "# 1. get the corpus\n",
        "wikiner_corpus: Corpus = WIKINER_ENGLISH().downsample(0.1)\n",
        "print(wikiner_corpus)\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = Corpus.make_tag_dictionary( wikiner_corpus, tag_type='ner')\n",
        "print(tag_dictionary.idx2item)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-23 15:57:25,013 Reading data from /root/.flair/datasets/wikiner_english\n",
            "2021-05-23 15:57:25,016 Train: /root/.flair/datasets/wikiner_english/aij-wikiner-en-wp3.train\n",
            "2021-05-23 15:57:25,019 Dev: None\n",
            "2021-05-23 15:57:25,021 Test: None\n",
            "Corpus: 11514 train + 1279 dev + 1422 test sentences\n",
            "[b'<unk>', b'O', b'B-MISC', b'E-MISC', b'S-PER', b'S-LOC', b'B-ORG', b'I-ORG', b'E-ORG', b'S-ORG', b'I-MISC', b'B-PER', b'I-PER', b'E-PER', b'S-MISC', b'B-LOC', b'E-LOC', b'I-LOC', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W4UKrp-7uw8",
        "outputId": "a6e9def3-c33e-4477-f083-408c5161c16b"
      },
      "source": [
        "print(wikiner_corpus.train[73])\n",
        "print(wikiner_corpus.train[73].to_tagged_string())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"Scholars include the linguist and Boasian Edward Sapir .\"   [− Tokens: 9  − Token-Labels: \"Scholars <NNS> include <VBP> the <DT> linguist <NN> and <CC> Boasian <JJ/S-PER> Edward <NNP/B-PER> Sapir <NNP/E-PER> . <.>\"]\n",
            "Scholars <NNS> include <VBP> the <DT> linguist <NN> and <CC> Boasian <JJ/S-PER> Edward <NNP/B-PER> Sapir <NNP/E-PER> . <.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-nRr9Q8g1T"
      },
      "source": [
        "Ok, above, we loaded a corpus, a collection of texts, and with this collection the annotation of these texts.<br>\n",
        "Next, we load the data, we prepared using Spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD2GaEJN76XT",
        "outputId": "242e67de-6d13-49d4-bd45-4bb202c20207"
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "downsample = 0.1 # 1.0 is full data, try a much smaller number like 0.01 to test run the code\n",
        "data_folder = os.getcwd()\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
        "                                                             train_file='training_data.csv',\n",
        "                                                             test_file='test_data.csv',\n",
        "                                                           dev_file=None).downsample(downsample)\n",
        "print(corpus)\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
        "print(tag_dictionary.idx2item)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-23 15:41:30,800 Reading data from /content/gdrive/My Drive/flair\n",
            "2021-05-23 15:41:30,804 Train: /content/gdrive/My Drive/flair/training_data.csv\n",
            "2021-05-23 15:41:30,808 Dev: None\n",
            "2021-05-23 15:41:30,810 Test: /content/gdrive/My Drive/flair/test_data.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 798 train + 89 dev + 303 test sentences\n",
            "[b'<unk>', b'O', b'I-Degree', b'B-Location', b'I-Location', b'L-Location', b'B-Skills', b'I-Skills', b'L-Skills', b'U-Location', b'U-Degree', b'U-Skills', b'B-Degree', b'L-Degree', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzpjKKrkwWFq"
      },
      "source": [
        "__Question 2__: what is the difference between __tag_dictionary__ created in the cell above, and __tag_dictionary__ created before that.<br>\n",
        "__Answer 2__: The wiki dictionary has E and S suffixed tags which are semantically the same as L and U.<br>\n",
        "\n",
        "\n",
        "Next, we take the first sentence from the test data, and annotate this sentence using __to_tagged_string__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nie8-FX99JB7",
        "outputId": "8083553d-4a89-4ebc-daa8-bf94c86f650d"
      },
      "source": [
        "for sent in corpus.test:\n",
        "  print(sent.to_tagged_string())\n",
        "  break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Willing to relocate to : Bengaluru <U-Location> , Karnataka WORK EXPERIENCE Principal Engineer Technical Staff Company 1 - Bengaluru <U-Location> , Karnataka - September 2005 to Present Total Experience : 12 years 6 months .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrVyGQFwWFs"
      },
      "source": [
        "__Question 3__: Why is not every word annotated?<br>\n",
        "How do you explain the difference to the result from __to_tagged_string__ applied to one sentence from the wiki ner corpus?<br>\n",
        "__Answer 3__: Not every word corresponds to a tag. The wiki ner corpus also assigned tags which are part of the POS-tagging."
      ]
    }
  ]
}